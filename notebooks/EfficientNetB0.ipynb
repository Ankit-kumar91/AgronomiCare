{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNetB0\n",
    "### Database: [Plant Diseases Training Dataset](https://www.kaggle.com/datasets/nirmalsankalana/plant-diseases-training-dataset/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import nessesary packages, libraries and global variables\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from  PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.utils import split_dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import sys\n",
    "sys.path.append('../modeling')\n",
    "\n",
    "RSEED = 42\n",
    "dataset_path = '../data/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_split_train_val(data_path):\n",
    "    ''' \n",
    "    Function needs filefath as parameter, it will create a validation dataset of 20% of the total df, \n",
    "    Needs an RSEED as global variable,\n",
    "    Image will be cropped to 1:1 and altered to 224 x 224\n",
    "    '''\n",
    "    image = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_path, \n",
    "        validation_split = 0.2,\n",
    "        subset = \"both\", \n",
    "        seed = RSEED,\n",
    "        image_size = (224, 224),\n",
    "        crop_to_aspect_ratio = True,\n",
    "        label_mode = 'categorical'\n",
    "    )\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = load_preprocess_split_train_val(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check an example \n",
    "val_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Developing a model\n",
    "------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "model = keras.applications.EfficientNetB0(\n",
    "    include_top=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get overview of the model architecture\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def build_model(num_classes):\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)) # Define the input layer with the shape of input images\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\") # Load the EfficientNetB0 model pretrained on ImageNet without the top classification layer\n",
    "                                                                                        # Use the input layer defined above\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output) # Apply global average pooling to the output of the base layers\n",
    "    x = layers.BatchNormalization()(x) # Apply batch normalization to normalize the activations of the previous layer\n",
    "\n",
    "\n",
    "    top_dropout_rate = 0.2 # Define the dropout rate\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x) # Apply dropout regularization to the previous layer\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x) # Add a dense layer for classification with softmax activation\n",
    "\n",
    "    # Compile\n",
    "    model = keras.Model(inputs, outputs, name=\"EfficientNet\") # Construct the final model with the specified input and output layers\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-2) # Define the optimizer with a learning rate of 0.01 using the Adam optimizer\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(num_classes=39)\n",
    "\n",
    "epochs = 12  # @param {type: \"slider\", min:8, max:80}\n",
    "#hist = model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next we want to unfreeze a couple of layers and retrain with own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we want to unfreeze 10 layers and retrain \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def unfreeze_model_and_clone(model):\n",
    "    # Clone the original model\n",
    "    unfrozen_model = tf.keras.models.clone_model(model)\n",
    "    unfrozen_model.set_weights(model.get_weights())  # Copy weights\n",
    "\n",
    "    # Unfreeze the top 10 layers while leaving BatchNorm layers frozen\n",
    "    for layer in unfrozen_model.layers[-10:]:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5)\n",
    "    unfrozen_model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return unfrozen_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with unfrozen layers\n",
    "unfrozen_model2 = unfreeze_model_and_clone(model)\n",
    "\n",
    "epochs = 8\n",
    "hist = unfrozen_model2.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
    "\n",
    "plot_hist(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with unfrozen layers\n",
    "unfrozen_model3 = unfreeze_model_and_clone(model)\n",
    "\n",
    "epochs = 12\n",
    "hist3 = unfrozen_model3.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
    "\n",
    "# Save the model to disk\n",
    "unfrozen_model3.save(\"unfrozen_model3.h5\")\n",
    "\n",
    "plot_hist(hist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(hist3.history['loss'])\n",
    "plt.plot(hist3.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the specified directory\n",
    "model_dir = \"../models/\"\n",
    "model_filename = \"efficient_unfrozen_12.h5\"\n",
    "unfrozen_model3.save(os.path.join(model_dir, model_filename))\n",
    "\n",
    "# You can load it back with keras.models.load_model()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Testing of the model with unseen data\n",
    "------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_test(data_path):\n",
    "    ''' \n",
    "    Function needs filepath as parameter, it will create a validation dataset of 20% of the total df, \n",
    "    Needs an RSEED as global variable,\n",
    "    Image will be cropped to 1:1 and altered to 224 x 224\n",
    "    '''\n",
    "    image_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_path,\n",
    "        image_size = (224, 224),\n",
    "        crop_to_aspect_ratio = True,\n",
    "        label_mode = 'categorical',\n",
    "        shuffle = False\n",
    "    )\n",
    "    return image_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_path = '../data/test/'\n",
    "\n",
    "test_ds = load_test(dataset_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check an example \n",
    "test_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset\n",
    "predictions = unfrozen_model3.predict(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Plotting the results and getting evaluation metrics\n",
    "------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the true labels from the test dataset\n",
    "y_true = []\n",
    "for filepath in test_ds.file_paths:\n",
    "    label = os.path.basename(os.path.dirname(filepath))\n",
    "    y_true.append(label)\n",
    "\n",
    "# Extract unique class labels from your training data\n",
    "classes = sorted(set(y_true))\n",
    "\n",
    "# Step 2: Convert true labels to indices using the same mapping used during training\n",
    "class_to_index = {cls: i for i, cls in enumerate(classes)}\n",
    "y_true_indices = np.array([class_to_index[label] for label in y_true])\n",
    "\n",
    "# Step 3: Use your model to make predictions on the test dataset\n",
    "y_pred_probabilities = unfrozen_model3.predict(test_ds)\n",
    "\n",
    "# Step 4: Convert the predicted class probabilities to class labels\n",
    "y_pred_indices = np.argmax(y_pred_probabilities, axis=1)\n",
    "y_pred = [classes[i] for i in y_pred_indices]\n",
    "\n",
    "# Step 5: Generate the classification report\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix using seaborn heatmap with green color palette\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Greens\", xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we should try with the suffle parameter turned on (while loarding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Retrain with augmented Data\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with augmented training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define your data augmentation parameters\n",
    "# Define your data augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.8, 1.2],  # Adjust brightness by random factor between 0.8 and 1.2\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "data_gen = image.ImageDataGenerator(\n",
    "    # define the preprocessing function that should be applied to all images\n",
    "    preprocessing_function=preprocess_input,\n",
    "    # fill_mode='nearest',\n",
    "    # rotation_range=20,\n",
    "    # width_shift_range=0.2,\n",
    "    # height_shift_range=0.2,\n",
    "    # horizontal_flip=True, \n",
    "    # zoom_range=0.2,\n",
    "    # shear_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Define the directory containing your saved model\n",
    "model_dir = \"../models/\"\n",
    "\n",
    "# Specify the filename of your saved model\n",
    "model_filename = \"efficient_unfrozen_12.h5\"\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(os.path.join(model_dir, model_filename))\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_augmentation_layers = [\n",
    "    layers.RandomRotation(factor=0.15),\n",
    "    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "    layers.RandomFlip(),\n",
    "    layers.RandomContrast(factor=0.1),\n",
    "]\n",
    "\n",
    "def img_augmentation(images):\n",
    "    for layer in img_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "TARGET_SIZE = (224,224,3)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, featurewise_center=True, \n",
    "    featurewise_std_normalization=True, \n",
    "    rotation_range=20, width_shift_range=0.2,\n",
    "    height_shift_range=0.2, horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./224)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '../data/train',\n",
    "    target_size=TARGET_SIZE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    '../data/test',\n",
    "    target_size=TARGET_SIZE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_aug = loaded_model.fit(train_generator, steps_per_epoch=int(2400/32), epochs=10, \n",
    "          validation_data=validation_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
