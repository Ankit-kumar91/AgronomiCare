{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from  PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import split_dataset\n",
    "\n",
    "RSEED = 42\n",
    "dataset_path = '../data/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import data\n",
    "\n",
    "data = []\n",
    "\n",
    "# Specify the path to your dataset\n",
    "dataset_path = '../data/images/'\n",
    "\n",
    "# Iterate through each plant folder\n",
    "for plant_class in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, plant_class)\n",
    "    \n",
    "    # Iterate through each image in the plant folder\n",
    "    for image_file in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_file)\n",
    "        data.append({'Image_Path': image_path, 'Class': plant_class})\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of classes\n",
    "class_counts = df['Class'].value_counts()\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.countplot(y='Class', data=df, order=class_counts.index)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract plant types from class names\n",
    "df['plant_type'] = df['Class'].apply(lambda x: x.split('___')[0])\n",
    "\n",
    "# Count the occurrences of each plant type\n",
    "plant_type_counts = df['plant_type'].value_counts()\n",
    "\n",
    "# Display the count of each plant type\n",
    "print(plant_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_rows = df[df['Class'].apply(lambda x: len(x.split('___')) < 2)]\n",
    "print(\"Problematic Rows:\")\n",
    "print(problematic_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#problematic_rows = df[df['Class'].str.contains('Grape_leaf_blight')]\n",
    "df['Class'] = df['Class'].replace('Grape_leaf_blight', 'Grape___leaf_blight', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract disease types from class names\n",
    "df['disease_type'] = df['Class'].apply(lambda x: x.split('___')[1])\n",
    "\n",
    "# Count the occurrences of each disease type\n",
    "disease_type_counts = df['disease_type'].value_counts()\n",
    "\n",
    "# Display the count of each disease type\n",
    "print(disease_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disease_type'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display sample images from each class\n",
    "'''\n",
    "class_folders = os.listdir(dataset_path)\n",
    "for class_folder in class_folders:\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    sample_image = os.listdir(class_path)[0]\n",
    "    image_path = os.path.join(class_path, sample_image)\n",
    "    # Display the image\n",
    "    img = Image.open(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Class: {class_folder}\")\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(df['disease_type']).tolist()\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, train_data = ts.keras.utils.split_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_train_data(data_path):\n",
    "    ''' \n",
    "    Function needs filefath as parameter, it will create a training dataset of 80% of the total df, \n",
    "    Needs an RSEED as global variable,\n",
    "    Image will be cropped to 1:1 and altered to 224 x 224\n",
    "    '''\n",
    "    image = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_path, \n",
    "        validation_split = 0.3,\n",
    "        subset = \"training\", \n",
    "        seed = RSEED,\n",
    "        image_size = (224, 224),\n",
    "        crop_to_aspect_ratio = True,\n",
    "    )\n",
    "    return image\n",
    "    #image = tf.image.flip_left_right(image)\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_preprocess_train_data(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of test_ds:\", type(train_ds))\n",
    "\n",
    "dataset_shape = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "print(\"Shape of test_ds:\", dataset_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_validation_data_and_test(data_path):\n",
    "    ''' \n",
    "    Function needs filefath as parameter, it will create a testing dataset of 20% of the total df, \n",
    "    Needs an RSEED as global variable,\n",
    "    Image will be cropped to 1:1 and altered to 224 x 224\n",
    "    '''\n",
    "    image = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_path, \n",
    "        validation_split = 0.3,\n",
    "        subset = \"validation\", \n",
    "        seed = RSEED,\n",
    "        image_size = (224, 224),\n",
    "        crop_to_aspect_ratio = True\n",
    "    )\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = load_preprocess_validation_data_and_test(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_from_val(val_ds):\n",
    "    image = tf.keras.utils.split_dataset(val_ds, left_size=0.5, shuffle=True, seed=RSEED)\n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = extract_test_from_val(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
    "#val_ds = prepare(val_ds)\n",
    "#test_ds = prepare(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.EfficientNetB0(\n",
    "    include_top=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing of the weights in order not to retrain\n",
    "\n",
    "model.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (224, 224, 3))\n",
    "\n",
    "base = model(inputs)\n",
    "\n",
    "flatten = GlobalAveragePooling2D()(base)\n",
    "\n",
    "outputs = Dense(61, activation='softmax')(flatten)\n",
    "\n",
    "model_enB0 = Model(inputs,outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_enB0.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_np = np.asarray(train)\n",
    "#train_y = np.asarray(train_y)\n",
    "#test_np = np.asarray(test)\n",
    "#validation_y = np.asarray(validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_enB0.fit(train,\n",
    "          verbose=2, # how the training log should get printed \n",
    "          epochs=10,\n",
    "          validation_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def extract_test_from_val(val_ds):\n",
    "    # Shuffle the validation dataset\n",
    "    val_ds = val_ds.shuffle(buffer_size=len(val_ds), seed=RSEED)\n",
    "\n",
    "    # Split the validation dataset into two subsets\n",
    "    test_ds = val_ds.take(len(val_ds) // 2)\n",
    "    \n",
    "    return test_ds\n",
    "\n",
    "# Example usage\n",
    "# Assuming val_ds is your validation dataset\n",
    "test_ds = extract_test_from_val(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_ds is your TensorFlow dataset object\n",
    "# You can check the type of the dataset\n",
    "print(\"Type of test_ds:\", type(test_ds))\n",
    "\n",
    "# You can check the shape of the dataset\n",
    "dataset_shape = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "print(\"Shape of test_ds:\", dataset_shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
