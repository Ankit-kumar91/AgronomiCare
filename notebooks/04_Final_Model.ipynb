{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tried all kinds of different pre-trained models and tuning methods, it's finally time to assemble our very own model based on what we've learned. This model will then serve as the foundation for our application that will be deployed using streamlit.\n",
    "\n",
    "*Note: To get the exact same model, please follow the steps in the previous notebooks, and pay special attention to the merged and deleted classes. If you're just looking for the model file, it's located at `models/\n",
    "model_filtered.h5`.*\n",
    "\n",
    "If you want to use the app right aways, you can do so with `streamlit run main.py`. This will use the model provided with this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../functions')\n",
    "from preprocessing import img_dataset_from_dir_and_split_train_val \n",
    "from modeling import build_model, unfreeze_model_and_clone\n",
    "from plot_functions import plot_training_metrics\n",
    "\n",
    "RSEED = 42\n",
    "DATASET_PATH = '../data/images/' # Path to the parent folder where the original data is stored\n",
    "TRAINING_IMAGES = '../data/train' # Path to training images\n",
    "TESTING_IMAGES = '../data/test' # Path to testing images\n",
    "CLASSES = [\n",
    "    'alternaria_leaf_spot',\n",
    "    'bacterial_blight',\n",
    "    'bacterial_spot',\n",
    "    'bacterial_wilt',\n",
    "    'black_measles',\n",
    "    'black_rot',\n",
    "    'blast',\n",
    "    'brown_spot',\n",
    "    'brown_streak_disease',\n",
    "    'citrus_greening',\n",
    "    'common_rust',\n",
    "    'early_blight',\n",
    "    'gray_leaf_spot',\n",
    "    'healthy',\n",
    "    'isariopsis_leaf_spot',\n",
    "    'late_blight',\n",
    "    'leaf_curl',\n",
    "    'leaf_mold',\n",
    "    'mosaic_disease',\n",
    "    'northern_leaf_blight',\n",
    "    'powdery_mildew',\n",
    "    'red_rot',\n",
    "    'spider_mites',\n",
    "    'target_spot',\n",
    "    'tungro',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Before working with Keras, it's good practice to clear the session\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Next, the train-validation-split takes place. The train-test-split has already been done at the end of notebook 1\n",
    "train_ds, val_ds = img_dataset_from_dir_and_split_train_val(TRAINING_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. We define the model and exclude the top layer\n",
    "\n",
    "model = keras.applications.EfficientNetB0(\n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "# Optional: Take a look at the model's architecture\n",
    "# mode.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Here we build the model with 25 classes\n",
    "model = build_model(num_classes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Unfreeze the top 10 layers\n",
    "unfrozen_model = unfreeze_model_and_clone(model)\n",
    "\n",
    "epochs = 10\n",
    "hist = unfrozen_model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Let's have a look at our metrics\n",
    "plot_training_metrics(hist)\n",
    "\n",
    "# ... and heatmap\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix using seaborn heatmap with green color palette\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Greens\", xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. We then have a look at the classification report\n",
    "\n",
    "def load_test(data_path):\n",
    "    ''' \n",
    "    Function needs filepath as parameter, it will create a validation dataset of 20% of the total df, \n",
    "    Needs an RSEED as global variable,\n",
    "    Image will be cropped to 1:1 and altered to 224 x 224\n",
    "    '''\n",
    "    image_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_path,\n",
    "        image_size = (224, 224),\n",
    "        crop_to_aspect_ratio = True,\n",
    "        label_mode = 'categorical',\n",
    "        shuffle = False\n",
    "    )\n",
    "    return image_dataset \n",
    "\n",
    "dataset_test_path = '../data/filtered_diseases/test_filtered/'\n",
    "\n",
    "test_ds = load_test(dataset_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the true labels from the test dataset\n",
    "y_true = []\n",
    "for filepath in test_ds.file_paths:\n",
    "    label = os.path.basename(os.path.dirname(filepath))\n",
    "    y_true.append(label)\n",
    "\n",
    "# Extract unique class labels from your training data\n",
    "classes = sorted(set(y_true))\n",
    "\n",
    "# Step 2: Convert true labels to indices using the same mapping used during training\n",
    "class_to_index = {cls: i for i, cls in enumerate(classes)}\n",
    "y_true_indices = np.array([class_to_index[label] for label in y_true])\n",
    "\n",
    "# Step 3: Use your model to make predictions on the test dataset\n",
    "y_pred_probabilities = patho_model_10.predict(test_ds)\n",
    "\n",
    "# Step 4: Convert the predicted class probabilities to class labels\n",
    "y_pred_indices = np.argmax(y_pred_probabilities, axis=1)\n",
    "y_pred = [classes[i] for i in y_pred_indices]\n",
    "\n",
    "# Step 5: Generate the classification report\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. We can have a look at the different class probabilities\n",
    "\n",
    "def display_class_probabilities(model, img_path, class_names):\n",
    "    # Load and preprocess the input data\n",
    "    img = image.load_img(img_path, target_size=(224, 224)) \n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Get class probabilities\n",
    "    probabilities = model.predict(img_array)[0]\n",
    "\n",
    "    # Display class probabilities with class names\n",
    "    for class_name, prob in zip(class_names, probabilities):\n",
    "        print(f\"{class_name}: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_class_probabilities(model_filtered.h5, image_path, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Once we're happy with the model, we save it\n",
    "unfrozen_model.save('patho_model_10.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
